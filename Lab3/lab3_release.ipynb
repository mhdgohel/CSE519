{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac53911-016c-4abb-98f9-4b613bde3b66",
   "metadata": {},
   "source": [
    "# General instructions for all labs\n",
    "\n",
    "1. To turn in:\n",
    " - this python notebook, filled out (2 pts)\n",
    " - a *standalone* PDF report that contains all the plots, and the answers to all the discussion questions (2 pts)\n",
    "\n",
    "2. Use of ChatGPT / CoPilot / etc:\n",
    "   - Allowed, but you own everything that is generated\n",
    "   - This means that any part of the solution can be asked in the quiz. It can be as detailed as \"What was the batch size you used in training\" or specific as \"what exactly does masking do in this case?\" Any discussion question is also game for a quiz question.\n",
    "   - If I find AI usage to be excessive. I can individually drag any of you in for a 1-1 meeting, in which I grill you on your code. If it looks like irresponsible copy/pasting, without proper understanding, I reserve the right to drastically lower your grade, or even submit cases to GGAC for ethical review.\n",
    "  \n",
    "3. Use of peer collaboration:\n",
    "   - In general not allowed. (Discussion / comparing answers is ok, but work on actual coding independently.)\n",
    "   - Exceptions can be made if you all wrote your own training script, but 1. it takes forever to train or 2. you don't have great compute resources. Then you can share a trained model amongst yourself *and declare it on your pdf*. However, the code for training *still must be written by yourself*\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a58353-287f-4cd0-a8f5-750887ddabb1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Lab 3: Scalable Retrieval with Project Gutenberg\n",
    "\n",
    "Modern AI systems don’t just generate answers — they rely on **retrieval** to find the right context before generation. Whether you’re building a search engine, a recommendation system, or a Retrieval-Augmented Generation (RAG) pipeline, the quality and speed of retrieval often determine how useful the final system will be.\n",
    "\n",
    "In this lab, you’ll explore the core ideas of **scalable** retrieval using Project Gutenberg texts. Books are long, hierarchical, and varied — which makes them an excellent testbed for learning how to index, search, and evaluate retrieval aton, or just keep it high-level so the reranker is an assignment step?\n",
    "tudents can *see* the difference between the two structures?\n",
    "letions spelled out as mini-exercises)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556a93b-1940-424f-abb4-211c1156936a",
   "metadata": {},
   "source": [
    "## 📚 The Project Gutenberg Corpus\n",
    "\n",
    "For this lab, we will be working with texts from **Project Gutenberg**, a large collection of public-domain books. Project Gutenberg has been digitizing and distributing literary works since 1971, making it one of the oldest and largest open digital libraries. The collection spans over **70,000 works**, ranging from classic literature to historical documents.\n",
    "\n",
    "Because the raw Project Gutenberg site can be tricky to scrape (inconsistent file formats, encodings, and compression), we’ll use a cleaned dataset hosted on Kaggle:\n",
    "\n",
    "👉 [Project Gutenberg – Over 70,000 Books (Kaggle Dataset)](https://www.kaggle.com/datasets/jasonheesanglee/gutenberg-over-70000)\n",
    "\n",
    "This version provides a consolidated, reproducible collection of the corpus, which avoids many of the Unicode and file format errors students encounter when downloading directly from gutenberg.org.\n",
    "\n",
    "We’ll use this dataset as our **source corpus** for chunking, embedding, and retrieval experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcb03d-0b5f-43ba-9173-3ccb69783922",
   "metadata": {},
   "source": [
    "## Part 1: Download and Inspect the Data\n",
    "\n",
    "1. **Download the dataset**\n",
    "\n",
    "   * Use the Kaggle link above: [Project Gutenberg – Over 70,000 Books](https://www.kaggle.com/datasets/jasonheesanglee/gutenberg-over-70000).\n",
    "   * Unzip the dataset into a directory on your computer. You should see:\n",
    "\n",
    "     * Many book files in `.pkl` format (each file contains the text of a book).\n",
    "     * A metadata file called `gutenberg_over_70000_metadata.csv`.\n",
    "\n",
    "2. **Inspect the book files and metadata**\n",
    "\n",
    "   * Scripts are provided to walk through the directory and preview the contents of `.pkl` files (e.g., first 100 characters or tokens).\n",
    "   * The metadata file contains details such as book ID, title, and author.\n",
    "   * Run them and look around a bit.\n",
    "\n",
    "3. **Load the data**\n",
    "   * You will extend these inspection scripts to combine the book text and metadata into a single data structure:\n",
    "\n",
    "     * **Key:** book number (ID)\n",
    "     * **Value:** dictionary with both the text and metadata details.\n",
    "    **You may not want to load all the data at once, as that is very memory intensive. Just have a natural break point at, say, 1000 books, which you can remove later once we decide to modify this function to be more efficient.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c4fdd-73a1-488e-8352-e1944b20eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_metadata(root_dir):\n",
    "    filepath = os.path.join(root_dir, 'gutenberg_over_70000_metadata.csv')\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        print(\"\\nFilepath:\", filepath)\n",
    "        print(df.head(10))          # Show first 10 rows\n",
    "        print(\"Total rows:\", len(df))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {filepath}: {e}\")\n",
    "    \n",
    "def preview_pkl_files(root_dir):\n",
    "    \"\"\"\n",
    "    Traverse root_dir and subdirectories, open .pkl files, \n",
    "    and print a preview of the first 100 characters or words.\n",
    "    \"\"\"\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                filepath = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    with open(filepath, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    print('\\n',filepath, data[:10],len(data))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {filepath}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "load_metadata('archive')\n",
    "#preview_pkl_files(\"archive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bcf20-fe98-45a2-966a-4821845f0bcb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 📖 From Text to Embeddings\n",
    "\n",
    "Note that actually loading the raw text of all the books is extremely cumbersome, and also not that useful for retrieval. Instead, for retrieval systems, we don’t search directly over words — we search in **embedding space**, where each passage of text is represented as a dense vector.\n",
    "\n",
    "Why do we need **chunking**?\n",
    "\n",
    "* Embedding models have a **fixed input size** (typically 512–1024 tokens). A whole book is far too long to fit.\n",
    "* Even if a model could process the entire book, a single embedding would dilute meaning — specific details would get lost.\n",
    "* By splitting the book into smaller, overlapping passages (e.g. 500 words with 50 words overlap), we keep each chunk semantically coherent and within the model’s capacity.\n",
    "\n",
    "Why do we need **aggregation**?\n",
    "\n",
    "* If we only keep passage-level embeddings, retrieval becomes more expensive (millions of vectors).\n",
    "* Sometimes we want a **compact representation** of the entire book (for catalog search, or as a coarse filter before drilling down).\n",
    "* Aggregating embeddings — for example by **averaging** all chunk embeddings — provides this compact representation while still reflecting the book’s overall content.\n",
    "* Averaging is a simple and widely used method: it balances the contributions of all chunks without blowing up dimensionality. Other methods (max pooling, weighted averaging, multi-vector representations) exist, but averaging is the most practical starting point.\n",
    "\n",
    "---\n",
    "--\n",
    "\n",
    "Since entire books are far too long to embed directly, we need to:\n",
    "\n",
    "1. **Chunk** the book into smaller passages (e.g. \\~500 words each, with overlap).\n",
    "2. **Embed** each passage using a pretrained model.\n",
    "3. **Aggregate** the embeddings into a single book-level vector (e.g. by averaging).\n",
    "\n",
    "Computing embeddings for all chunks can be expensive. To make this feasible, we will **subsample** by taking only the first *N* chunks (e.g. 000) per book. This reduces memory and runtime while still giving a representative eme students?\n",
    "\n",
    "\n",
    "\n",
    "### Your Task\n",
    "\n",
    "* First, look over the two functions offered to you below. You can use them, modify them, or rewrite your own functions. If you use these ones, please fill out the prompts in the comments, as you could be asked about it later.\n",
    "* Next, modify your loading function from above to load the embedding computed from the text, rather than the entire text itself. You should notice that memory does not grow nearly as large.\n",
    "\n",
    "Once it is all loaded, you can save it in a pkl file, to be easily loaded in the future.\n",
    "\n",
    "This step can take a while. You may not want to do it last minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7aba41-8035-403e-b9bd-40a6a48e5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50, max_num_chunks = 10):\n",
    "    \"\"\"\n",
    "    What does this function do?\n",
    "\n",
    "    What is the input?\n",
    "\n",
    "    What is the output?\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words) and len(chunks) < max_num_chunks:\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # slide window with overlap\n",
    "    return chunks\n",
    "\n",
    "def embed_book(text, chunk_size=500, overlap=50, aggregate=\"mean\"):\n",
    "    \"\"\"\n",
    "    What does this function do?\n",
    "\n",
    "    What is the input?\n",
    "\n",
    "    What is the output?\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "    \n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\"\"\"\n",
    "# --- Example usage ---\n",
    "sample_text = books['167']['text']\n",
    "\n",
    "chunk_embeddings = embed_book(sample_text)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunk_embeddings))      # depends on chunk_size\n",
    "print(\"First chunk embedding shape:\", chunk_embeddings[0].shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769532a4-edbc-43ce-b551-9bd3ba8446d3",
   "metadata": {},
   "source": [
    "## Part 1 – Step 1: Build a Flat k-NN Graph\n",
    "\n",
    "1. **Pairwise Distances**\n",
    "\n",
    "   * In your notebook, write out the underlying equations for this computation.\n",
    "   * Comment on the computational and memory complexity of this approach (it is $O(m^2)$ in both time and space, where $m$ is the number of books).\n",
    "   * Make sure you understand the math — don’t just memorize it. I may ask you to reproduce it.\n",
    "\n",
    "2. **Visualization with PCA**\n",
    "\n",
    "   * Use PCA to reduce the embeddings to 2D, and plot the books in this space.\n",
    "   * Instead of labeling points with IDs, annotate them with their book titles from the metadata. For example:\n",
    "\n",
    "     ```python\n",
    "     plt.text(embedding_2d[i, 0], embedding_2d[i, 1], book_embs[label]['metadata']['Book Title'], fontsize=6, alpha=0.7)\n",
    "     ```\n",
    "   * Comment on the geometry and clustering of these visual embeddings. Do they make sense?\n",
    "\n",
    "3. **Nearest Neighbor Graph**\n",
    "\n",
    "   * Create a nearest neighbor graph using `networkx`.\n",
    "   * Each node corresponds to a book embedding.\n",
    "   * You may connect nodes in one of two ways:\n",
    "\n",
    "     * **Fixed degree (k-NN):** Connect each node to its $d$ closest neighbors, where $d$ is a hyperparameter (e.g. 5, 10, 20).\n",
    "     * **Distance threshold:** Connect each node to any other node whose distance is less than a threshold $\\varepsilon$, for example the 15th percentile of all pairwise distances.\n",
    "   * Instead of plotting the raw graph with all its edges (which quickly becomes unreadable), visualize the graph structure using its adjacency matrix.\n",
    "\n",
    "4. **Discussion**\n",
    "\n",
    "   * Comment on the pros and cons of constructing an NN graph in each of the two ways.\n",
    "   * How sensitive is the graph structure to the choice of $d$ or $\\varepsilon$?\n",
    "   * Compare how graph connectivity changes with the hyperparameter (clusters vs. isolated nodes).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074a9e4-7bf6-4b1f-825a-4ef41770907e",
   "metadata": {},
   "source": [
    "## Part 1 Step 2. **Perform NN search over the graph**\n",
    "\n",
    "1. Given a **query vector**, traverse the graph to find the nearest neighbors.  \n",
    "\n",
    "2. First, perform **exact search**:  \n",
    "   * Embed the query.  \n",
    "   * Compute squared distances to all book embeddings.  \n",
    "   * Find and print the 5 nearest neighbors.  \n",
    "   * Use Google (or another source) to verify whether these books are actually related to your query summary.  \n",
    "\n",
    "3. Next, try an **approximate search** using *NN descent*:  \n",
    "   * Start from a random node.  \n",
    "   * Look for the neighbor that is closest to the query node.  \n",
    "   * Keep moving until you can’t find a closer neighbor.  \n",
    "   * Return the path of visited nodes and the final result.  \n",
    "\n",
    "4. **Discussion:**  \n",
    "   * What is the computational complexity of finding the exact *d* neighbors for each query?  \n",
    "   * How could approximate nearest neighbors reduce this complexity?  \n",
    "   * What problems can occur with approximate NN (e.g., local optima)?  \n",
    "   * How could we bypass these problems (e.g., by using more than one random seed)?  \n",
    "\n",
    "5. **Visualization (optional):** Plot the query and its connected neighbors on your PCA projection of the embeddings.\n",
    "\n",
    "```\n",
    "\n",
    "# --- Query text ---\n",
    "query = \"\"\"In a near-future world where borders constantly shift due to climate change \n",
    "and political unrest, a young cartographer named Elara is recruited by an international \n",
    "coalition to create the first “living map” — a dynamic atlas that redraws itself in real time. \n",
    "As she works, she discovers that the map doesn’t just reflect the world, but begins to shape it: \n",
    "cities vanish from memory, coastlines retreat overnight, and entire communities are erased when \n",
    "they no longer appear on her charts. Torn between her duty to her employers and her conscience, \n",
    "Elara sets out on a journey across collapsing nations to uncover who is really controlling the \n",
    "map — and whether she can break the paradox before the world is redrawn beyond recognition.\"\"\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b55bc-352e-4c77-8dc8-1f04138e3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time, tracemalloc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def profile(fn, fn_name, *args, **kwargs):\n",
    "    print('profile',fn_name)\n",
    "    tracemalloc.start()\n",
    "    t0 = time.perf_counter()\n",
    "    result = fn(*args, **kwargs)   # run the function\n",
    "    t1 = time.perf_counter()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    print(f\"{fn_name} took {t1 - t0:.4f}s, memory peak {peak/1e6:.2f} MB\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc23ab8-c56f-49a3-92f4-9ac9e6bf1890",
   "metadata": {},
   "source": [
    "## Part 1 Step 3. **Add and Delete Nodes Online**\n",
    "\n",
    "In real systems, data is not static — new documents (or books) arrive, and old ones may be removed. A good retrieval system must be able to **update its index online** without rebuilding everything from scratch.  \n",
    "\n",
    "1. **Exact insertion and deletion**  \n",
    "   * Insert a new node by computing its distance to **all existing nodes**.  \n",
    "     - Find its nearest neighbors (e.g. top-k or within an ε-threshold).  \n",
    "     - Connect it to those neighbors in the graph.  \n",
    "   * Delete a node by removing it and repairing its neighbors.  \n",
    "     - If nodes lose important edges (e.g. drop below degree k), recompute distances among affected neighbors to re-establish the k-NN property.  \n",
    "   * **Question:** What is the computational complexity of inserting or deleting nodes *exactly* in this way? How does this scale as the graph grows?  \n",
    "\n",
    "2. **Approximate insertion and deletion**  \n",
    "   * Instead of comparing against all nodes:  \n",
    "     - Start from a random entry node.  \n",
    "     - Traverse the graph greedily, moving to neighbors closer to the new node.  \n",
    "     - Use the visited nodes as candidates for edges.  \n",
    "   * For deletion, simply remove the node and its edges without full repair, letting future approximate traversals fill in missing connections.  \n",
    "   * **Question:** Where do the computational reductions come from in this approximate version? What trade-offs are introduced (e.g. possible missed neighbors, local optima)?  \n",
    "\n",
    "3. **Discussion**  \n",
    "   * Why is online add/delete functionality important in real retrieval systems?  \n",
    "   * Compare the costs of exact vs approximate methods. In what scenarios would exact updates be necessary, and when is approximation acceptable?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65fd7b-0504-4b2c-8c2d-4e20416e3d5d",
   "metadata": {},
   "source": [
    "## Part 2: **Hierarchical graphs**\n",
    "\n",
    "\n",
    "So far, you have built and experimented with a **flat nearest neighbor graph**.  \n",
    "Now we will move to a more scalable structure: the **Hierarchical Navigable Small-World (HNSW) graph**.  \n",
    "\n",
    "### 🔎 What is HNSW?\n",
    "HNSW is a graph-based data structure for **approximate nearest neighbor (ANN) search**.  \n",
    "It extends the idea of a flat k-NN graph into a **multi-level hierarchy**, inspired by skip lists:  \n",
    "- Each element is assigned a *maximum layer* at random.  \n",
    "- Higher layers are sparse (few nodes, long-range connections).  \n",
    "- Lower layers are dense (many nodes, local connections).  \n",
    "- Searching starts at the top, then proceeds down through the levels, refining at each stage.  \n",
    "\n",
    "This combination of *random layering* and *graph connectivity* yields very efficient nearest neighbor search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cab642-f811-4cb9-9d71-9772f7eff848",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 Step 1: Construction of the HNSW Graph\n",
    "1. **Random Level Assignment**  \n",
    "   * Each node is assigned a maximum level \\( L \\), sampled from an exponential distribution.  \n",
    "   * Most nodes only appear at the lowest level, while a few appear in many levels (acting as \"hubs\").  \n",
    "\n",
    "2. **Layered Graphs**  \n",
    "   * For each level \\( \\ell \\), connect nodes to their nearest neighbors (just like in the flat k-NN graph).  \n",
    "   * Higher levels have fewer nodes, so connections there act like \"express lanes.\"  \n",
    "   * Lower levels are more detailed, ensuring accuracy.  \n",
    "\n",
    "3. **Hierarchy**  \n",
    "   * The result is a stack of graphs: top layers are sparse and global, bottom layers are dense and local.  \n",
    "\n",
    "\n",
    "\n",
    "I have given you a backbone of an HNSW that includes the build function. Answer the following questions.\n",
    "\n",
    " - Which layers is the densist?\n",
    " - Which layer contains the entry point?\n",
    " - What does the method sample_level return for a node? What is m_L? at graph?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cec7f-515e-4ea9-81ce-7489eeb0597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "class HNSW:\n",
    "    def __init__(self, max_m=5, m_L=1.0, seed=42):\n",
    "        \"\"\"\n",
    "        Simplified HNSW-style graph for teaching.\n",
    "        \"\"\"\n",
    "        self.max_m = max_m\n",
    "        self.m_L = m_L\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.layers = []             # list of graphs, one per layer\n",
    "        self.node_levels = {}        # mapping: node_id -> max level\n",
    "        self.embeddings = None       # store embeddings\n",
    "        self.next_id = 0             # assign new IDs sequentially\n",
    "        self.entry_point = None      # node ID at top level\n",
    "        self.current_max_level = -1  # highest level index so far\n",
    "\n",
    "    def sample_level(self):\n",
    "        \"\"\"Sample the maximum level for a node (exponential distribution).\"\"\"\n",
    "        u = self.rng.random()\n",
    "        return int(-np.log(u) * self.m_L)\n",
    "\n",
    "    def build(self, embeddings):\n",
    "        \"\"\"Build HNSW from scratch.\"\"\"\n",
    "        m = embeddings.shape[0]\n",
    "        self.embeddings = embeddings.copy()\n",
    "        levels = [self.sample_level() for _ in range(m)]\n",
    "        max_level = max(levels) if m > 0 else -1\n",
    "        self.current_max_level = max_level\n",
    "\n",
    "        # Create one graph per level\n",
    "        self.layers = [nx.Graph() for _ in range(max_level + 1)]\n",
    "\n",
    "        for i in range(m):\n",
    "            self.node_levels[i] = levels[i]\n",
    "            for L in range(levels[i] + 1):\n",
    "                self.layers[L].add_node(i)\n",
    "\n",
    "        # Connect nodes at each level\n",
    "        for L in range(max_level + 1):\n",
    "            nodes_at_L = [i for i in range(m) if levels[i] >= L]\n",
    "            if len(nodes_at_L) <= 1:\n",
    "                continue\n",
    "\n",
    "            X = embeddings[nodes_at_L]  # shape (n_L, d)\n",
    "            # Pairwise squared distances\n",
    "            D = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * X @ X.T\n",
    "            np.fill_diagonal(D, np.inf)\n",
    "\n",
    "            nearest_matrix = np.argsort(D, axis=1)[:, :self.max_m]\n",
    "            for row_idx, i in enumerate(nodes_at_L):\n",
    "                for j_local in nearest_matrix[row_idx]:\n",
    "                    j = nodes_at_L[j_local]\n",
    "                    self.layers[L].add_edge(i, j, weight=float(D[row_idx, j_local]))\n",
    "\n",
    "        # Choose entry point as any node at top level\n",
    "        if m > 0:\n",
    "            self.entry_point = max(range(m), key=lambda nid: levels[nid])\n",
    "        else:\n",
    "            self.entry_point = None\n",
    "\n",
    "        self.next_id = m\n",
    "        return self.layers, self.node_levels\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba15c0-9683-4127-b094-7a42c4fe8a08",
   "metadata": {},
   "source": [
    "## Part 2 step 2\n",
    "\n",
    "Next, add the following class functions:\n",
    "\n",
    "### hnsw.search\n",
    "1. **Start from the top layer** (which has the fewest nodes).  \n",
    "2. **Greedy search**: at each step, move to the neighbor that is closest to the query.  \n",
    "3. When no closer neighbor exists, **drop down one level** and continue.  \n",
    "4. At the lowest layer, refine until the nearest neighbor (or set of neighbors) is found.  \n",
    "\n",
    "This ensures a fast global-to-local search process.\n",
    "\n",
    "---\n",
    "\n",
    "### hnsw.insert\n",
    "1. **Sample a level** for the new node.  \n",
    "2. **Search**:  \n",
    "   * Start from the top layer and traverse greedily to find a close entry point.  \n",
    "   * At each level down to the node’s maximum level, continue refining its nearest neighbors.  \n",
    "3. **Connect**:  \n",
    "   * Add the new node to each of its assigned layers.  \n",
    "   * Link it to its nearest neighbors in those layers (ensuring max degree is not exceeded).  \n",
    "\n",
    "This way, new nodes integrate into both global and local structures.\n",
    "\n",
    "---\n",
    "\n",
    "### hnsw.delete\n",
    "1. **Remove the node** from all layers in which it appears.  \n",
    "2. **Reconnect neighbors** if necessary:  \n",
    "   * At each layer, consider the deleted node’s neighbors.  \n",
    "   * Re-link them if their degree drops below the required minimum or connectivity is at risk.  \n",
    "3. In practice, many implementations avoid explicit repair and rely on redundancy in the graph to maintain accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Your Task\n",
    "- Extend your code to use an **HNSW-style hierarchical graph** instead of a flat one.  \n",
    "- Practice **traversal, insertion, and deletion** in this setting.  \n",
    "- Compare the **search paths** between flat graphs and HNSW graphs:  \n",
    "  * How many nodes are visited?  \n",
    "  * How does the path length differ?  \n",
    "  * How does the random layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad2907-b7e5-4d50-a323-bb8042f13601",
   "metadata": {},
   "source": [
    "### Questions for you\n",
    "\n",
    "- **Layer assignment strategy:**  \n",
    "  How are levels assigned to each node under this exponential scheme?  Is a node assigned to just one layer, or multiple layers? Explain the exact assignment scheme. \n",
    "  What are the advantages and disadvantages of this method compared to deterministic or uniform assignments?  \n",
    "\n",
    "- **Computational and memory bottlenecks:**  \n",
    "  Where do the main costs arise (construction, traversal, updates)?  \n",
    "  At which stages are these bottlenecks reduced or avoided?  \n",
    "  Why is this hierarchical design favored over a single flat graph?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c5a2f3-94f5-47e8-99b6-9caaec9df428",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Scalable Retrieval with FAISS\n",
    "\n",
    "Now that you’ve built your own HNSW structure, it’s time to see how it compares to industry-strength libraries. FAISS (Facebook AI Similarity Search) is a widely used toolkit for nearest neighbor retrieval at scale. In this part, you will explore FAISS’s **built-in indexing methods** and evaluate how they differ from your own implementation.\n",
    "\n",
    "1. **Indexing Approaches**\n",
    "\n",
    "   * **Flat Index (Exact Search):**\n",
    "\n",
    "     ```python\n",
    "     import faiss\n",
    "     d = embeddings.shape[1]   # embedding dimension\n",
    "     index_flat = faiss.IndexFlatL2(d)   # exact search\n",
    "     index_flat.add(embeddings)          # add all vectors\n",
    "     D, I = index_flat.search(embeddings[:5], k=5)  # search first 5 queries\n",
    "     ```\n",
    "\n",
    "   * **HNSW Index (Approximate Search):**\n",
    "\n",
    "     ```python\n",
    "     index_hnsw = faiss.IndexHNSWFlat(d, 32)  # 32 = max neighbors per node\n",
    "     index_hnsw.hnsw.efConstruction = 40      # build-time parameter\n",
    "     index_hnsw.add(embeddings)\n",
    "     D_hnsw, I_hnsw = index_hnsw.search(embeddings[:5], k=5)\n",
    "     ```\n",
    "\n",
    "2. **Comparison**\n",
    "\n",
    "   * **Computation:** How does query speed change between the flat index and FAISS HNSW as the dataset size grows?\n",
    "   * **Memory:** How much extra memory is used to store the graph structure in FAISS HNSW compared to the flat index?\n",
    "   * **Quality:** How close are the retrieved neighbors from FAISS HNSW to the ground-truth neighbors found by exact search? (Measurectly measure the speed difference?\n",
    "and recommendation)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
