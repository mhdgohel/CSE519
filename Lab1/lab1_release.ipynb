{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f041a859-2418-4649-a868-a4688c902166",
   "metadata": {},
   "source": [
    "# General instructions for all labs\n",
    "\n",
    "1. To turn in:\n",
    " - this python notebook, filled out (2 pts)\n",
    " - a *standalone* PDF report that contains all the plots, and the answers to all the discussion questions (2 pts)\n",
    "\n",
    "2. Use of ChatGPT / CoPilot / etc:\n",
    "   - Allowed, but you own everything that is generated\n",
    "   - This means that any part of the solution can be asked in the quiz. It can be as detailed as \"What was the batch size you used in training\" or specific as \"what exactly does masking do in this case?\" Any discussion question is also game for a quiz question.\n",
    "   - If I find AI usage to be excessive. I can individually drag any of you in for a 1-1 meeting, in which I grill you on your code. If it looks like irresponsible copy/pasting, without proper understanding, I reserve the right to drastically lower your grade, or even submit cases to GGAC for ethical review.\n",
    "  \n",
    "3. Use of peer collaboration:\n",
    "   - In general not allowed. (Discussion / comparing answers is ok, but work on actual coding independently.)\n",
    "   - Exceptions can be made if you all wrote your own training script, but 1. it takes forever to train or 2. you don't have great compute resources. Then you can share a trained model amongst yourself *and declare it on your pdf*. However, the code for training *still must be written by yourself*\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05def5e6-cb90-4b71-9d25-c38900e3ca72",
   "metadata": {},
   "source": [
    "## **Lab 1 Overview**\n",
    "\n",
    "This lab explores how domain-specific training shapes NanoGPT’s internal representations and output behavior. You will train three separate *small* NanoGPT models on distinct, **tiny** corpora (each \\~50–200 KB of text):\n",
    "\n",
    "1. **Shakespeare corpus** – Short subset of public-domain plays.\n",
    "2. **Wikipedia corpus** – Small scraped/cleaned sample from open Wikipedia dumps.\n",
    "3. **Math textbook corpus** – Small public-domain math primer or textbook excerpt.\n",
    "\n",
    "By comparing these models across zero-shot and few-shot settings, you will observe how the same generative architecture adapts its “classification” over the vocabulary depending on the training distribution.\n",
    "\n",
    "\n",
    "## **Learning Outcomes**\n",
    "\n",
    "By the end of the lab, students should:\n",
    "\n",
    "1. Recognize that **generation is token-level classification**, and that domain-specific data shifts the learned classification boundaries.\n",
    "2. Understand **zero-shot vs. few-shot** generalization in the context of specialized generative models.\n",
    "3. Interpret **softmax confidence** and detect overconfidence/oversaturation phenomena.\n",
    "4. Apply **model interpretability tools** (Grad-CAM) to compare how domain training changes token importance.\n",
    "5. Connect all observations to **overfitting, evaluation metrics, and embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b35d5-0e25-4a3c-9dfa-b9b156bd437c",
   "metadata": {},
   "source": [
    "## **Part 1 – Training Domain-Specific Models**\n",
    "\n",
    "* Use the **same small NanoGPT config** (e.g., `n_layer=2`, `n_head=2`, `n_embd=128`, `block_size=64`) for all runs.\n",
    "* Train on CPU or free Colab GPU — each model should converge in minutes given the small corpora and reduced parameters.\n",
    "* Monitor:\n",
    "\n",
    "  * Training loss curves.\n",
    "  * Qualitative performance of generated samples.\n",
    "\n",
    "This section mostly requires you to run scripts that are already written. At each step, pay close attention to the comments, as you may be asked about them in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a4a45-117c-4481-a59c-8eac4bc4c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, zipfile, requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Choose the corpus\n",
    "corpus_name = \"shakespeare\"  # Change to \"shakespeare\", \"wikipedia\", or \"math\"\n",
    "\n",
    "# Reliable sources:\n",
    "# - shakespeare: Karpathy's tiny Shakespeare (plain text)\n",
    "# - wikipedia: https://www.kaggle.com/datasets/ffatty/plain-text-wikipedia-simpleenglish\n",
    "# - math: https://archive.org/stream/CalculusMadeEasy/Calculus_Made_Easy_Thompson_djvu.txt\n",
    " \n",
    "\n",
    "\n",
    "# Download and show sample\n",
    " \n",
    "text = Path(f\"data/{corpus_name}.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "print(f\"Corpus length: {len(text)} characters\")\n",
    "print(\"Sample:\")\n",
    "print(text[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b64e8-d63f-41c6-a49a-be1fb0e7812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Prepare dataset for character-level modeling\n",
    "# -----------------------------------------\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, block_size, stoi=None, itos=None):\n",
    "        \"\"\"\n",
    "        text: The raw text string we want to train on.\n",
    "        block_size: The length of each training sequence (number of characters).\n",
    "        stoi, itos: Optional vocab mappings. If provided, reuse them.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Build or reuse the vocabulary\n",
    "        if stoi is None or itos is None:\n",
    "            # Build from scratch\n",
    "            self.chars = sorted(list(set(text)))\n",
    "            self.vocab_size = len(self.chars)\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        else:\n",
    "            # Reuse given vocab\n",
    "            self.stoi = stoi\n",
    "            self.itos = itos\n",
    "            self.vocab_size = len(self.stoi)\n",
    "\n",
    "        # 2. Store sequence length\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # 3. Encode dataset into indices\n",
    "        #    Use .get(ch, 0) so unknown characters map to 0\n",
    "        self.data = torch.tensor([self.stoi.get(c, 0) for c in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "block_size = 64\n",
    "text = Path(f\"data/{corpus_name}.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "dataset = CharDataset(text, block_size=block_size, stoi=None, itos=None)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b142c8-03a5-44d3-9d64-fbe144992322",
   "metadata": {},
   "source": [
    " \n",
    "## Analysis: Character Frequency Histogram\n",
    "\n",
    "Visualize the differences in vocabulary and character usage between corpora (`shakespeare`, `wikipedia`, `math`). Do this by creating a histogram of each character's frequency of occurance, for each corpora.\n",
    "  \n",
    "Unicode safety\n",
    "\n",
    "   * Some characters may not render properly in plots or Jupyter.\n",
    "   * Replace problematic characters with their Unicode code point.\n",
    "   * Example helper function:\n",
    "\n",
    "     ```python\n",
    "     def safe_label(c):\n",
    "         try:\n",
    "             c.encode(\"ascii\")  # check if ASCII-printable\n",
    "             return c\n",
    "         except UnicodeEncodeError:\n",
    "             return f\"U+{ord(c):04X}\"\n",
    "     ```\n",
    "\n",
    " \n",
    "   * Generate one histogram for each corpus (`shakespeare`, `wikipedia`, `math`).\n",
    "   * For each corpus, write **1–2 sentences** describing:\n",
    "\n",
    "     * Which characters are most common.\n",
    "     * Any unusual symbols or formatting.\n",
    "     * How the distributation of the corpus represents a distribution of a more general language model, e.g. one learned by ChatGPT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61374b77-65dd-45e8-8332-8edbb38fd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanogpt_model import GPT  # Import the GPT model class from the NanoGPT repo\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Define a configuration object for the GPT model\n",
    "# -----------------------------------------\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size,\n",
    "                 n_layer=2, n_head=2, n_embd=128, dropout=0.0, bias = True):\n",
    "        \"\"\"\n",
    "        This class is a container for all the key hyperparameters\n",
    "        that define the size and shape of the GPT model.\n",
    "        We pass this config into the GPT constructor so the model\n",
    "        can be built with these exact settings.\n",
    "\n",
    "        ---------------------------\n",
    "        Parameter meanings:\n",
    "        ---------------------------\n",
    "\n",
    "        vocab_size:\n",
    "            - Number of unique tokens in our dataset.\n",
    "            - In a character-level model, this is the number of distinct characters.\n",
    "            - Maps directly to the size of the token embedding matrix:\n",
    "                token_embedding_table.shape == (vocab_size, n_embd)\n",
    "\n",
    "        block_size:\n",
    "            - Maximum context length (sequence length) the model sees at once.\n",
    "            - Sets the width of the positional embedding table:\n",
    "                position_embedding_table.shape == (block_size, n_embd)\n",
    "            - Also defines the mask size in self-attention so the model only attends\n",
    "              to the last `block_size` tokens.\n",
    "\n",
    "        n_layer:\n",
    "            - Number of Transformer blocks stacked in the model.\n",
    "            - Each block = (Multi-Head Self-Attention + Feedforward MLP + LayerNorm).\n",
    "            - More layers allow the model to capture more complex dependencies.\n",
    "\n",
    "        n_head:\n",
    "            - Number of attention heads per multi-head attention layer.\n",
    "            - Each head learns to focus on different positions/tokens in the sequence.\n",
    "            - Heads are concatenated then projected back into `n_embd` dimensions.\n",
    "\n",
    "        n_embd:\n",
    "            - Dimensionality of token embeddings and all hidden states.\n",
    "            - Controls the \"width\" of the model.\n",
    "            - Affects:\n",
    "                * Token embedding size\n",
    "                * Positional embedding size\n",
    "                * Per-head dimension in attention: head_dim = n_embd / n_head\n",
    "                * Hidden size of the feedforward layers in each block.\n",
    "\n",
    "        dropout:\n",
    "            - Dropout probability applied in various places (attention, MLP) during training.\n",
    "            - Helps prevent overfitting by randomly zeroing activations.\n",
    " \n",
    "        \"\"\"\n",
    "\n",
    "        # Store all parameters for use by the GPT class\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout \n",
    "        self.bias = bias  \n",
    "\n",
    "\n",
    "config = GPTConfig(vocab_size=dataset.vocab_size, block_size=block_size)\n",
    "model = GPT(config)\n",
    "print(model)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907ec6d-eb79-4bcf-9c82-60c7edda4dc1",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## Analysis: Model Architecture Diagram\n",
    " \n",
    "Create a clear, labeled diagram of the NanoGPT model you trained, showing the **flow of data** from input to output, including the **shapes/dimensions** at each stage. Do this *by hand*. You may use a tablet, but it cannot look code-generated. \n",
    "\n",
    " \n",
    "1. **Overall Layout**\n",
    "\n",
    "   * Show the full pipeline:\n",
    "     **Input Tokens → Token Embedding → Positional Embedding → Transformer Blocks → LayerNorm → Output Head → Softmax**.\n",
    "\n",
    "2. **Transformer Blocks**\n",
    "\n",
    "   * Each block should include:\n",
    "\n",
    "     * **LayerNorm 1 → Multi-Head Causal Self-Attention → Residual Connection**\n",
    "     * **LayerNorm 2 → Feedforward MLP → Residual Connection**\n",
    "   * Clearly label these sub-components and indicate that this block repeats `n_layer` times.\n",
    "\n",
    "3. **Dimensions**\n",
    "\n",
    "   * Label the **shape** of the tensor at each stage (batch size = `B`, sequence length = `T`, embedding size = `n_embd`):\n",
    "\n",
    "     * Input: `(B, T)` (integer token IDs)\n",
    "     * Token embeddings: `(B, T, n_embd)`\n",
    "     * After adding positional embeddings: `(B, T, n_embd)`\n",
    "     * Inside attention: queries/keys/values → `(B, n_head, T, head_dim)`\n",
    "     * MLP layers: `(B, T, 4*n_embd)` then back to `(B, T, n_embd)`\n",
    "     * Output logits: `(B, T, vocab_size)`\n",
    "\n",
    "4. **Connections**\n",
    "\n",
    "   * Draw arrows between each component to show the data flow.\n",
    "   * Mark residual connections that add the block’s input to its output.\n",
    "\n",
    "5. **Label Hyperparameters**\n",
    "\n",
    "   * Include:\n",
    "\n",
    "     * `n_layer` = number of transformer blocks\n",
    "     * `n_head` = number of attention heads per block\n",
    "     * `n_embd` = embedding dimension\n",
    "     * `block_size` = maximum context length\n",
    "     * `vocab_size` = size of the token vocabulary\n",
    "\n",
    "6. **Final Output**\n",
    "\n",
    "   * Show that the model outputs logits for each token position, then a softmax over the vocabulary to get probabilities.\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad171451-66fc-4aa2-8cb8-4394324a18df",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## Training NanoGPT\n",
    "\n",
    "The following code actually **trains NanoGPT** on your selected dataset.\n",
    "Your job is to:\n",
    "\n",
    "1. **Play around with the hyperparameters** (e.g. learning rate, batch size, context length, number of layers).\n",
    "2. **Run the training loop** long enough to get meaningful results.\n",
    "3. **Save a trained checkpoint for each of the three corpora** (`shakespeare`, `wikipedia`, `math`).\n",
    " \n",
    "After training:\n",
    "\n",
    "* You should have a folder of checkpoints (e.g. `checkpoints/shakespeare/final.pt`).\n",
    "* Each corpus will give you a slightly different “voice” when you generate text.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e4182-8208-469c-90c1-593cdf96f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "print(corpus_name)\n",
    "# Directory for this corpus\n",
    "ckpt_dir = os.path.join(\"checkpoints\", corpus_name)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "final_ckpt_path = os.path.join(ckpt_dir, \"final.pt\")\n",
    "\n",
    "if os.path.exists(final_ckpt_path):\n",
    "    print(f\"Final checkpoint for '{corpus_name}' found at {final_ckpt_path}.\")\n",
    "    ckpt = torch.load(final_ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    losses = ckpt.get(\"losses\", [])\n",
    "    print(f\"Loaded model with {len(losses)} stored loss values.\")\n",
    "else:\n",
    "    print(f\"No checkpoint found for '{corpus_name}', starting training...\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    max_iters = 5000\n",
    "\n",
    "    losses = []   # will now store only full-batch eval losses\n",
    "    model.train()\n",
    "    for it in range(max_iters):\n",
    "        xb, yb = next(iter(loader))\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            # Evaluate on full dataset (avg loss across all batches)\n",
    "            model.eval()\n",
    "            eval_losses = []\n",
    "            \n",
    "            max_batches = len(loader) * 0.01\n",
    "            for i, (xb_eval, yb_eval) in enumerate(loader):\n",
    "                if i >= max_batches:  break\n",
    "                logits_eval, loss_eval = model(xb_eval, yb_eval)\n",
    "                eval_losses.append(loss_eval.item())\n",
    "            avg_loss = torch.mean(torch.tensor(eval_losses))\n",
    "            losses.append(avg_loss)\n",
    "            print(f\"Iter {it}/{max_iters}, Avg Loss over dataset: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save periodic checkpoint\n",
    "            iter_ckpt_path = os.path.join(ckpt_dir, f\"iter_{it}.pt\")\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"config\": config.__dict__,\n",
    "                \"itos\": dataset.itos,\n",
    "                \"stoi\": dataset.stoi,\n",
    "                \"losses\": losses\n",
    "            }, iter_ckpt_path)\n",
    "            print(f\"Saved checkpoint: {iter_ckpt_path}\")\n",
    "            model.train()  # back to training mode\n",
    "\n",
    "    # Save final checkpoint\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"config\": config.__dict__,\n",
    "        \"itos\": dataset.itos,\n",
    "        \"stoi\": dataset.stoi,\n",
    "        \"losses\": losses\n",
    "    }, final_ckpt_path)\n",
    "    print(f\"Saved final model to {final_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a646a96-f161-4783-8db1-46115ddfebc3",
   "metadata": {},
   "source": [
    "## Analysis: Track Model Output Over Training\n",
    "\n",
    "\n",
    "### **Procedure**\n",
    "* Extract model checkpoints every **100 iterations** (e.g., `iter_100.pt`, `iter_200.pt`, `iter_300.pt`, …).\n",
    "* For each corpus (`shakespeare`, `wikipedia`, `math`):\n",
    " * Use the provided generation code to produce **one line of generated text** from the **same fixed prompt** (e.g., `\"Once upon a time\"`).\n",
    " * If you find the output has not yet achieved the langauge structure quality you desire, you may want to train it for longer. Go until you have a strong model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Consistency**\n",
    "\n",
    "* Keep the **prompt** identical across all checkpoints, but consistent to the corpora in mind.\n",
    "* Use the same **number of generated tokens** each time.\n",
    "* This ensures you can make direct comparisons between outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Reporting**\n",
    "\n",
    "In your lab report:\n",
    "\n",
    "* Include a short **table or list** for each corpus showing:\n",
    "\n",
    "  * Iteration number.\n",
    "  * The generated output (first \\~100 characters).\n",
    "\n",
    "* Discuss the following:\n",
    "\n",
    "  * At what point does the text start to look coherent?\n",
    "  * How does the style differ between Shakespeare, Wikipedia, and Math?\n",
    "  * Does the model become **overconfident** (repetitive text, fixed patterns) at later stages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072ae49-7391-4d66-aa64-72c6eb31a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This code loads a specific training checkpoint of NanoGPT\n",
    "# (saved during training) and uses it to generate text.\n",
    "\n",
    "\n",
    "def generate_small_text(corpus, start_text, iteration_to_load, max_new_tokens):\n",
    "    # Path to the specific checkpoint\n",
    "    \n",
    "    if iteration_to_load == \"final\":\n",
    "        ckpt_path = os.path.join(\"checkpoints\", corpus, \"final.pt\")\n",
    "    else:\n",
    "        ckpt_path = os.path.join(\"checkpoints\", corpus, f\"iter_{iteration_to_load}.pt\")\n",
    "    \n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"Checkpoint {ckpt_path} not found. Make sure to train first.\")\n",
    "    else:\n",
    "        # Load checkpoint from disk\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "        # Rebuild the GPT model from saved config\n",
    "        loaded_config = type(\"GPTConfig\", (), ckpt[\"config\"])\n",
    "        gen_model = GPT(loaded_config)\n",
    "        gen_model.load_state_dict(ckpt[\"model_state\"])\n",
    "        gen_model.eval()\n",
    "    \n",
    "        # Tokenization: convert characters to IDs\n",
    "        stoi = ckpt[\"stoi\"]\n",
    "        itos = ckpt[\"itos\"]\n",
    "        idx = torch.tensor([stoi.get(c, 0) for c in start_text], dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "        # Autoregressive generation loop\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Feed only the last block_size tokens\n",
    "                idx_cond = idx[:, -loaded_config.block_size:]\n",
    "                logits, _ = gen_model(idx_cond)\n",
    "    \n",
    "                # Convert logits to probabilities\n",
    "                probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    \n",
    "                # Sample next token from probability distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "                # Append new token to the sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "        # Decode IDs back into text\n",
    "        generated_text = ''.join([itos[i.item()] for i in idx[0]])\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c1cf5-b258-4cfc-a4e8-3fda93a23a08",
   "metadata": {},
   "source": [
    "## Analysis: Variability of the Final Model\n",
    "\n",
    "### **Procedure**\n",
    "\n",
    "* After training, take the **final checkpoint** for each corpus.\n",
    "* Using the same prompt as before (corpus-consistent), generate **multiple outputs (at least 5)**.\n",
    "* Each output should use the same number of tokens, so they are directly comparable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Consistency**\n",
    "\n",
    "* Keep the prompt identical across all generations for each corpus.\n",
    "* Do not change model parameters, only re-run generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Reporting**\n",
    "\n",
    "In your lab report:\n",
    "\n",
    "* Include the set of generated outputs from the final model.\n",
    "* Comment on:\n",
    "\n",
    "  * The **variance** across outputs: do they differ in interesting ways, or are they repetitive?\n",
    "  * Whether the variability seems to capture the **style** of the corpus.\n",
    "  * Whether you are satisfied with the outputs: does it feel like the model “learned” something meaningful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba797b96-e38e-4f27-b6e2-0b67933bfb71",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Analysis: Plot Training Loss from Checkpoints**\n",
    "\n",
    "* Produce **one loss curve plot per corpus** (`shakespeare`, `wikipedia`, `math`).\n",
    "* Compare and contrast the training behavior across the different corpora.\n",
    "\n",
    "**What to Comment On**\n",
    "\n",
    "* Identify which loss values seem to correspond to **qualitatively good performance** (e.g., when the model starts producing coherent, domain-appropriate text).\n",
    "* Note that the **initial drop in loss** is often not enough: real quality emerges only once the model reaches **very low error**.\n",
    "* Discuss differences across corpora:\n",
    "\n",
    "  * Which corpus needed lower loss values before outputs became convincing?\n",
    "  * Do some domains “sound good” ons for what to hand in)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e4482-5010-4c12-b9b5-3198b3995711",
   "metadata": {},
   "source": [
    "## **Part 2 – Cross-Domain Evaluation**\n",
    "\n",
    "* Evaluate each trained model **on each other’s dataset**:\n",
    "\n",
    "  * **Zero-shot**: Direct prompt without domain examples.\n",
    "  * **Few-shot**: Include 1–3 in-domain examples in the prompt.\n",
    "* Compare:\n",
    "\n",
    "  * Loss in-domain vs. out-of-domain.\n",
    "  * Quality and relevance of generated continuations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bf801-1cc8-48a0-89c4-e68b31613aa4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Detailed Instructions\n",
    "\n",
    "### **Part 2 Step 1: Zero-shot Evaluation**\n",
    "\n",
    "For each dataset pair (A, B) where A, B ∈ {`wikipedia`, `math`, `shakespeare`}:\n",
    "\n",
    "1. Load the **final checkpoint** of the model trained on A.\n",
    "\n",
    "   * When rebuilding the model, you must restore the **vocab size** from the checkpoint, e.g.\n",
    "\n",
    "     ```python\n",
    "     vocab_size = len(ckpt[\"stoi\"])\n",
    "     model = GPT(vocab_size=vocab_size)\n",
    "     ```\n",
    "   * This ensures the embedding and output layers match the original training.\n",
    "2. Evaluate it on dataset B **without in-domain examples**. Do so on 1% of the training data, as it is very large.\n",
    "3. Record the **loss values** in a 3×3 table (diagonal = in-domain).\n",
    "4. For each pair, include one **generated sample** with these instructions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89369e58-44f4-4e88-96ec-1a5e79e17344",
   "metadata": {},
   "source": [
    "### **Part 2 Step 2: Few-shot Evaluation**\n",
    "\n",
    "1. **Start from a Pretrained Model (Dataset A).**\n",
    "\n",
    "   * Choose a model that has already been trained on dataset **A** (e.g., `wikipedia`, `math`, `shakespeare`).\n",
    "   * Load the **final checkpoint** for A:\n",
    "\n",
    "     ```python\n",
    "     ckpt_path = f\"checkpoints/{A}/final.pt\"\n",
    "     ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "     ```\n",
    "   * Important: the vocabulary size used for training A may differ from the default.\n",
    "\n",
    "     * Before building the model, set\n",
    "\n",
    "       ```python\n",
    "       config.vocab_size = len(ckpt[\"stoi\"])\n",
    "       model = GPT(config)\n",
    "       ```\n",
    "     * This ensures the embedding and output layer dimensions match the checkpoint.\n",
    "   * Finally, load the weights:\n",
    "\n",
    "     ```python\n",
    "     model.load_state_dict(ckpt[\"model_state\"])\n",
    "     ```\n",
    "\n",
    "2. **Fine-tune on Dataset B.**\n",
    "\n",
    "   * Use the **same vocabulary (`stoi`, `itos`) from dataset A** to enco0de dataset B.\n",
    "   * Train for **200 iterations** with AdamW (`lr=**10, 25, 50, 100, 2ns thereafter if you extend training.\n",
    "   * Naming convention:\n",
    "\n",
    "     ```\n",
    "     {A}_start{N}_{B}_run{M}.pt\n",
    "     ```\n",
    "\n",
    "     where\n",
    "\n",
    "     * **A** = source dataset (e.g. `wikipedia`)\n",
    "     * **B** = target dataset (e.g. `math`)\n",
    "     * **N** = starting iteration (e.g. `0` if from scratch, or `500` if resuming)\n",
    "     * **M** = current fine-tuning iteration\n",
    "\n",
    "3. **Evaluate and Track Loss.**\n",
    "\n",
    "   * Every 50 steps, evaluate on a **1% subsample of dataset B** for speed.\n",
    "   * Record the average evaluation loss.\n",
    "   * Keep a running list of these evaluation losses to plot later.\n",
    "\n",
    "4. **Plot Loss Curves.**\n",
    "\n",
    "   * After training, plot **loss vs iteration** for each experiment.\n",
    "   * Compare curves across different (A→B) fine-tuning runs.\n",
    "   * This shows how quickly and effectively each pretrained model adapts to dataset B.\n",
    "\n",
    "5. **Examples**\n",
    "\n",
    "   * At each saved checkpoint (10, 25, 50, 100, 200), generate short text continuations from the model.\n",
    "   * Use the same sampling routine (temperature-controlled decoding).\n",
    "   * Collect and compare generations to see how quality improves over epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de420a16-84f4-40ab-86e5-892c26db36be",
   "metadata": {},
   "source": [
    "## **Part 2 Step 3: Few-shot from Earlier Checkpoints**\n",
    "\n",
    "* Repeat Step 2, but start from models that were **not fully trained** on A.\n",
    "* Compare results: does partial training make adaptation easier? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07141a0-4421-45f5-ad55-fc45a3c51d0a",
   "metadata": {},
   "source": [
    "### **Deliverables**\n",
    "\n",
    "* **Tables**: Zero-shot loss values (3×3).\n",
    "* **Plots**: Few-shot loss curves (per experiment).\n",
    "* **Samples**: Generated text at selected iterations.\n",
    "* **Discussion**: Briefly how to structure their zero-shot results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77a2a-156f-465c-aa62-6fb4f8539206",
   "metadata": {},
   "source": [
    "## **Part 3 – Inspecting the Softmax Output Layer**\n",
    "\n",
    "In this part, you will explore how the model’s **output probabilities** evolve during training for different prompts.\n",
    "\n",
    "### **What to Do**\n",
    "\n",
    "* Choose **several prompts** (at least 2–3) that differ in domain and style (e.g., math expressions, natural language, biology terms).\n",
    "* For each prompt, extract at each checkpoint:\n",
    "\n",
    "  * The **softmax probability distribution** over the vocabulary.\n",
    "  * The **entropy** of the distribution (a measure of model confidence/uncertainty).\n",
    "* Plot the **trajectories of top candidate tokens** over checkpoints for each prompt.\n",
    "\n",
    "### **What to Look For**\n",
    "\n",
    "* **Predictability & Hardening**\n",
    "\n",
    "  * Does the distribution concentrate (one token dominates) as training progresses?\n",
    "  * Are some prompts much more predictable than others?\n",
    "\n",
    "* **Entropy & Uncertainty**\n",
    "\n",
    "  * Compare entropy across prompts.\n",
    "  * Do in-domain prompts (e.g., math-related text) produce sharper, lower-entropy distributions?\n",
    "  * Do out-of-domain prompts produce flatter, high-entropy distributions?\n",
    "\n",
    "* **Biases in Token Predictions**\n",
    "\n",
    "  * Are certain tokens consistently high-probability regardless of prompt?\n",
    "  * Do different prompts encourage different sets of top tokens?\n",
    "\n",
    "### **What to Report**\n",
    "\n",
    "* Provide **plots** for each prompt.\n",
    "* Give a **written description** of what you observe:\n",
    "\n",
    "  * Where do probabilities harden?\n",
    "  * Where do they remain diffuse?\n",
    "  * How does this match your expectations about the model’s training data?\n",
    "* Discuss what your findings imply about the model’s **predictability and specialization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a635301-0f10-4af1-b4d4-fbe539ed8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "def load_model(ckpt_path,  GPT):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    config = type(\"GPTConfig\", (), ckpt[\"config\"])\n",
    "    stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
    "    config.vocab_size = len(stoi)\n",
    "    model = GPT(config)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    return model, stoi, itos\n",
    "\n",
    "def inspect_softmax(model, prompt, stoi, itos, top_k=10):\n",
    "    # pick a fallback index if <unk> not in vocab\n",
    "    unk_idx = stoi.get(\"<unk>\", next(iter(stoi.values())))  \n",
    "\n",
    "    x = torch.tensor([[stoi.get(c, unk_idx) for c in prompt]], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(model(x)[0][:, -1, :], dim=-1).squeeze()\n",
    "    entropy = -(probs * torch.log(probs + 1e-12)).sum().item()\n",
    "    top_p, top_i = torch.topk(probs, top_k)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"entropy\": entropy,\n",
    "        \"probs\": [(itos[i.item()], p.item()) for i, p in zip(top_i, top_p)]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae446e-4e9c-491f-b0d9-02d85638d2c2",
   "metadata": {},
   "source": [
    " \n",
    "## **Part 4 – Grad-CAM for Language Models**\n",
    "\n",
    "In this part, you’ll build a lightweight version of *Grad-CAM* for a language model. Instead of visualizing gradients in images, we’ll look at how gradients flow into **token embeddings** to measure which input tokens most affect a model’s probability of generating a target token.\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "1. **Compute gradients**\n",
    "\n",
    "   * Use the provided `token_gradients` function.\n",
    "   * This function backpropagates from the probability of a **target token** (e.g., `\"e\"`) back to the input embeddings.\n",
    "   * For each input token, it returns the gradient norm, which indicates **how much changing that token would affect the prediction**.\n",
    " \n",
    "2. **Experiment with checkpoints**\n",
    "\n",
    "   * Run the method across checkpoints of your trained model (e.g., every 100 iterations).\n",
    "   * Track how gradient importance changes over time.\n",
    "   * Try different prompts and different target tokens.\n",
    "\n",
    "3. **Compare across models**\n",
    "\n",
    "   * Run the same analysis on domain-trained models (e.g., trained on Wikipedia vs. math text).\n",
    "   * Compare: Do the same tokens consistently drive predictions? Or do models in different domains “care” about different tokens?\n",
    "\n",
    " \n",
    "### **Discussion Questions**\n",
    "\n",
    "1. **Character influence:**\n",
    "   Which characters consistently receive the highest gradient norms? Are they vowels, consonants, punctuation, or digits? Do frequent letters (like “e”) dominate, or do rare characters sometimes matter more?\n",
    "2. **Checkpoint dynamics:**\n",
    "   How does the distribution of influential tokens change as training progresses? Does the model become more focused or more diffuse?\n",
    "3. **Domain differences:**\n",
    "   Compare results from different domain-trained models. Do Wikipedia-trained models rely more on content words, while math-trained ones emphasize structure/symbols?\n",
    "4. **Zero-shot vs few-shot:**\n",
    "   If you add example completions to the prompt (few-shot), how does the gradient map shift compared to zero-shot?\n",
    "5. **Interpretability limits:**\n",
    "   What are the limitations of gradient-based methods? Do high gradient norms always mean high “importance”? When might this method be misleading?\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d340b4-7717-4a4f-a38f-b22f3458a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def token_gradients(model, prompt, target_token, stoi, itos ):\n",
    "    \"\"\"\n",
    "    Compute gradient norms per input token wrt probability of `target_token`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor([stoi[c] for c in prompt], dtype=torch.long)[None, :] \n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # Hook to capture embeddings with gradient\n",
    "    def save_grad(module, inp, out):\n",
    "        out.retain_grad()\n",
    "        grads['emb'] = out\n",
    "\n",
    "    handle = model.transformer.wte.register_forward_hook(save_grad)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, _ = model(x)\n",
    "    last_logits = logits[:, -1, :]   # [B, vocab]\n",
    "\n",
    "    target_idx = stoi.get(target_token, None)\n",
    "    if target_idx is None:\n",
    "        raise ValueError(f\"Token {target_token!r} not in vocab\")\n",
    "\n",
    "    probs = F.softmax(last_logits, dim=-1)\n",
    "    target_prob = probs[0, target_idx]\n",
    "\n",
    "    # Backprop\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    target_prob.backward()\n",
    "\n",
    "    # Now grads['emb'] has the embedding grads\n",
    "    grad_norms = grads['emb'].grad[0].norm(dim=-1)  # [T]\n",
    "    tokens = [itos[i.item()] for i in x[0]]\n",
    "\n",
    "    # Clean up the hook\n",
    "    handle.remove()\n",
    "\n",
    "    return list(zip(tokens, grad_norms.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce1fb3a9-7937-4fb3-a1e6-480388629a56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
