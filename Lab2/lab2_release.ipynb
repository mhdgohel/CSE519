{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9f9760-8b3e-4d96-946a-a26c1d0e94af",
   "metadata": {},
   "source": [
    "# General instructions for all labs\n",
    "\n",
    "1. To turn in:\n",
    " - this python notebook, filled out (2 pts)\n",
    " - a *standalone* PDF report that contains all the plots, and the answers to all the discussion questions (2 pts)\n",
    "\n",
    "2. Use of ChatGPT / CoPilot / etc:\n",
    "   - Allowed, but you own everything that is generated\n",
    "   - This means that any part of the solution can be asked in the quiz. It can be as detailed as \"What was the batch size you used in training\" or specific as \"what exactly does masking do in this case?\" Any discussion question is also game for a quiz question.\n",
    "   - If I find AI usage to be excessive. I can individually drag any of you in for a 1-1 meeting, in which I grill you on your code. If it looks like irresponsible copy/pasting, without proper understanding, I reserve the right to drastically lower your grade, or even submit cases to GGAC for ethical review.\n",
    "  \n",
    "3. Use of peer collaboration:\n",
    "   - In general not allowed. (Discussion / comparing answers is ok, but work on actual coding independently.)\n",
    "   - Exceptions can be made if you all wrote your own training script, but 1. it takes forever to train or 2. you don't have great compute resources. Then you can share a trained model amongst yourself *and declare it on your pdf*. However, the code for training *still must be written by yourself*\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2a234-4e63-455e-ae18-7b42fa175b60",
   "metadata": {},
   "source": [
    "# Lab 2: RAG over a Large Codebase\r\n",
    "\r\n",
    "## SciPy\r\n",
    "\r\n",
    "**SciPy** is a fundamental open-source library for scientific computing in Python. It builds on top of **NumPy** and provides efficient implementations of many core algorithms for mathematics, science, and engineering. SciPy is widely used in academia and industry for numerical analysis, optimization, signal processing, and more.\r\n",
    "\r\n",
    "### Key Features\r\n",
    "\r\n",
    "* **Linear Algebra & Optimization:** Robust solvers for systems of equations, eigenvalue problems, and constrained/unconstrained optimization.  \r\n",
    "* **Integration & Differential Equations:** Tools for numerical integration, ODE solvers, and quadrature.  \r\n",
    "* **Signal & Image Processing:** Filtering, Fourier transforms, and image manipulation utilities.  \r\n",
    "* **Statistics & Probability:** Random variables, hypothesis testing, and statistical distributions.  \r\n",
    "* **Sparse Matrices:** Efficient storage and computation with large, sparse systems.  \r\n",
    "\r\n",
    "SciPy underlies much of the Python scientific ecosystem, serving as the backbone for applications in physics, biology, engineering, data science, and machine learning.\r\n",
    "\r\n",
    "### Resources\r\n",
    "\r\n",
    "* 📖 [Documentation](https://docs.scipy.org/doc/scipy/)  \r\n",
    "* 💻 [GitHub Repository](https://github.com/scipy/scipy)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Lab Goal\r\n",
    "\r\n",
    "In this lab, you will build a **Retrieval-Augmented Generation (RAG) system** to answer natural language questions about a debase (\\~10,000 scripts). By combining a L and a RAG over the del (Ldocumentation and LM) rieved SciPy should help a newbie coder the system will help you explore and understand complex source code, algorithms, and APIs.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228b8d3-6f22-46ea-8062-973171948211",
   "metadata": {},
   "source": [
    "#\n",
    "\r\n",
    "## Part 1 – Setup and Preprocessing\r\n",
    "\r\n",
    "In this part, you will complete the following. The notebook will walk you through it step by step.\r\n",
    "\r\n",
    "1. **Documentation Preparation**\r\n",
    "\r\n",
    "   * Obtain the provided project documentation (manuals, reference guides, tutorials, etc.).\r\n",
    "   * Each section will be treated as text input for retrieval.\r\n",
    "   * Ignore large binary files, images, and other non-text resources.\r\n",
    "\r\n",
    "2. **Chunking**\r\n",
    "\r\n",
    "   * Split the documentation into overlapping chunks:\r\n",
    "\r\n",
    "     * Suggested: 300–500 tokens per chunk.\r\n",
    "     * Overlap: 50–100 tokens to preserve context across boundaries.\r\n",
    "   * Store metadata for each chunk, such as document name and section or page reference.\r\n",
    "\r\n",
    "3. **Embedding Generation**\r\n",
    "\r\n",
    "   * Use a sentence-transformer (e.g., `msmarco-distilbert-base-v3`) to generate embeddings for each chunk of documentation.\r\n",
    "   * Save embeddings and metadata to disk for reuse.\r\n",
    "\r\n",
    "4. **Vector Database**\r\n",
    "\r\n",
    "   * Load embeddings into FAISS or Chroma.\r\n",
    "   * Confirm you can perform a similarity search for a sampmake the flow of the lab smoother.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487bdb9-e17b-4175-ac7e-2e122eed4ff0",
   "metadata": {},
   "source": [
    "### Part 1 Step 1: Expanding Code Comments\n",
    "\n",
    "Take a look at each of these helper scripts. To demonstrate you fully understand the code, you will expand the comments.   The comments should explain not only *what* each line of code does, but also *why* it is there and *how* it contributes to the larger goal of the pipeline.\n",
    "\n",
    "You are welcome to use ChatGPT (or other tools) to help generate initial drafts of comments. However, you are ultimately responsible for understanding the details — you will be expected to **regurgitate this level of explanation on an exam without assistance**.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56938b71-6ac9-40d6-9b22-c77ef5d45fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b58504e435e4268a9ec3ede3cedc114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  32%|###1      | 724M/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a6f0d646ce47ffb2452a0342a3d03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8445578f741470c982f2c2c2813c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dfae279fb74de5b7a3f921b02a2c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02dd629ab7b44a1882f7323b0c0551e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21db554d07d748da832ab9bef010ecb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "models = {}\n",
    "#models['code'] = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v3\")\n",
    "models['code'] = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "models['docs'] = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# -------- 1. File Loader --------\n",
    "def load_code_files(base_dir, exts={\".cpp\", \".h\", \".c\", \".f90\", \".py\"}):\n",
    "    \"\"\"\n",
    "    What does it do? (1 sentence)\n",
    "\n",
    "\n",
    "    What are the inputs?\n",
    "    \n",
    "    What are the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for f in files:\n",
    "            if os.path.splitext(f)[-1] in exts:\n",
    "                try:\n",
    "                    path = Path(root) / f\n",
    "                    with open(path, \"r\", errors=\"ignore\") as fh:\n",
    "                        yield str(path), fh.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {f}: {e}\")\n",
    "\n",
    "\n",
    "# -------- 2. Chunker --------\n",
    "def chunk_text(text: str, chunk_size=1000, overlap=0):\n",
    "    \"\"\"\n",
    "    What does it do? (1 sentence)\n",
    "\n",
    "    \n",
    "    What are the inputs?\n",
    "    \n",
    "\n",
    "    What are the outputs?\n",
    "    \n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        piece = \" \".join(words[i:i+chunk_size])\n",
    "        if piece.strip():\n",
    "            chunks.append({\n",
    "                \"text\": piece,\n",
    "                \"start_word\": i,\n",
    "                \"end_word\": i+chunk_size\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------- 3. Embedding with CodeBERT --------\n",
    "def embed_texts(model, texts):\n",
    "    \"\"\"\n",
    "    What does it do? (1 sentence)\n",
    "\n",
    "    \n",
    "    What are the inputs?\n",
    "\n",
    "    \n",
    "    What are the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=16,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True  # good for cosine similarity\n",
    "    )\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856339c7-028b-4a05-a128-0d0ec16dfffd",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "## FAISS\r\n",
    "\r\n",
    "The next two code blocks introduce **FAISS** (Facebook AI Similarity Search), a library built for efficient similarity search across large collections of vectors. After we embed documentation chunks into high-dimensional vectors, FAISS gives us the data structures and algorithms needed to store them and quickly retrieve the closest matches to a query.\r\n",
    "\r\n",
    "A key feature of FAISS is its support for different types of **indexes** for vector search. In this lab, we will start with the simplest option: **IndexFlatL2**.\r\n",
    "\r\n",
    "* This index stores all vectors directly in memory.\r\n",
    "* It computes exact Euclidean ($L^2$) distances for each query.\r\n",
    "* It is best suited for datasets ranging from a few thousand up to a few hundred thousanke HNSW or IVF?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56424374-9265-4cfa-a4a8-b0c559ff188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 4. Build Vector DB --------\n",
    "def build_faiss_index(chunks_with_meta, dim):\n",
    "    \"\"\"\n",
    "    What does it do? (1 sentence)\n",
    "\n",
    "    \n",
    "    What are the inputs?\n",
    "    \n",
    "\n",
    "    What are the outputs?\n",
    "    \n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    embeddings = np.vstack([c[\"embedding\"] for c in chunks_with_meta]).astype(\"float32\")\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "# -------- Example Query --------\n",
    "def query_index(query, topk=5, code_or_doc = 'code'):\n",
    "    \"\"\"\n",
    "    What does it do? (1-2 sentences)\n",
    "    \n",
    "\n",
    "    What are the inputs?\n",
    "    \n",
    "\n",
    "    What are the outputs?\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    index = faiss.read_index(\"%s/code_chunks.index\" % code_or_doc)\n",
    "     \n",
    "    q_emb = embed_texts(models[code_or_doc],[query]).astype(\"float32\")\n",
    "    \n",
    "    distances, indices = index.search(q_emb, topk)\n",
    "        \n",
    "    return indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad7445-8fde-4dcd-b9b7-e9abe46edb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cc605d180149ebba4912e3ee736e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -------- MAIN PIPELINE --------\n",
    "def process_codebase(base_dir=\"scipy-main\", code_or_doc = 'code'):\n",
    "    all_chunks = []\n",
    "    if code_or_doc == 'docs': \n",
    "        exts = {'.rst'}\n",
    "        chunk_size = 1000\n",
    "    else:\n",
    "        exts = {'.py'}\n",
    "        chunk_size = 250\n",
    "    for fpath, text in load_code_files(base_dir, exts):\n",
    "        for chunk in chunk_text(text,chunk_size=chunk_size):\n",
    "            chunk[\"file\"] = fpath\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    # Embed with CodeBERT\n",
    "    texts = [c[\"text\"] for c in all_chunks]\n",
    "    \n",
    "    embs = embed_texts(models[code_or_doc], texts)\n",
    "    print('done with embed')\n",
    "    for c, e in zip(all_chunks, embs):\n",
    "        c[\"embedding\"] = e\n",
    "\n",
    "    # Save metadata\n",
    "    with open(\"%s/chunks_metadata.jsonl\" % code_or_doc, \"w\") as out:\n",
    "        for c in all_chunks:\n",
    "            meta = {k: v for k, v in c.items() if k != \"embedding\"}\n",
    "            out.write(json.dumps(meta) + \"\\n\")\n",
    "\n",
    "    # Build index\n",
    "    \n",
    "    dim = len(all_chunks[0][\"embedding\"])\n",
    "    index = build_faiss_index(all_chunks, dim)\n",
    "    faiss.write_index(index, \"%s/code_chunks.index\" % code_or_doc)\n",
    "    print(f\"Processed {len(all_chunks)} chunks from {base_dir}.\")\n",
    "\n",
    "\n",
    "#process_codebase(\"scipy-main/doc/source\",code_or_doc = 'docs')   # run once for docs\n",
    "process_codebase(\"scipy-main/scipy\",code_or_doc = 'code')   # run once for code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddbcb1-039b-463d-abe8-883da50eb24d",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Testing\n",
    "\n",
    "Now that you have built a vector-based retrieval tool on the **SciPy documentation**, the next step is to **qualitatively assess how well it works**. You will not compute numerical accuracy yet. Instead, you will explore retrieval behavior and record your judgments about when the system feels useful, misleading, or irrelevant.\n",
    "\n",
    "You will use two supports for this process:\n",
    "\n",
    "1. The **official SciPy documentation** (provided in the lab).\n",
    "2. A **free ChatGPT interface** to act as an external “relevance judge.”\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Design a Variety of Queries\n",
    "\n",
    "Create at least **5 queries** covering different types of information you might want from SciPy. Use the templates below and adapt them to optimization, statistics, linear algebra, and sparse matrices:\n",
    "\n",
    "* **Conceptual (high-level):**\n",
    "  Ask about an algorithm, method, or mathematical concept.\n",
    "  *Example:* `\"trust-region methods in optimization\"` or `\"sparse matrix factorization\"`.\n",
    "\n",
    "* **Function/API-specific:**\n",
    "  Target a known function, class, or module in SciPy.\n",
    "  *Example:* `\"scipy.optimize.minimize\"` or `\"scipy.sparse.linalg.cg\"`.\n",
    "\n",
    "* **Keyword-only:**\n",
    "  Use just a technical term.\n",
    "  *Example:* `\"Poisson distribution\"` or `\"LU decomposition\"`.\n",
    "\n",
    "* **Natural language (descriptive):**\n",
    "  Ask a full question in plain English.\n",
    "  *Example:* `\"How do I compute confidence intervals for a normal distribution in SciPy?\"`\n",
    "\n",
    "* **Edge cases:**\n",
    "  Include queries that are vague, misspelled, or off-topic.\n",
    "  *Example:* `\"optimizashun\"`, `\"solver\"`, `\"deep learning\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Compare with the Manual\n",
    "\n",
    "For each query:\n",
    "\n",
    "1. Look up the topic in the SciPy documentation (using search or Ctrl+F).\n",
    "2. Compare the retrieved snippets with the official docs.\n",
    "\n",
    "   * If they match or are consistent, mark as **aligned**.\n",
    "   * If they do not appear in the docs, mark as **potentially irrelevant**.\n",
    "\n",
    "This provides a rough ground truth based on trusted documentation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use ChatGPT as a Relevance Judge\n",
    "\n",
    "For an additional check, you will ask ChatGPT whether each retrieved snippet is relevant to the query. Use a **standardized prompt template** to keep your experiments consistent.\n",
    "\n",
    "**Template:**rence (optional): [short excerpt from SciPy documentation]  \n",
    "\n",
    "Question for ChatGPT:  \n",
    "\"Is this snippet relevant to the query? Answer Yes or No, and provide a one-sentence justification.\"  \n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Query: \"sparse matrix factorization\"  \n",
    "Snippet: \"The scipy.sparse.linalgecomposition of sparse matrices is available in scipy.sparse.linalg.\"  \n",
    "\n",
    "Question for ChatGPT:  \n",
    "\"Is this snippet relevant to the query? Answer Yes or No, and provide a one-sentence justification.\"  \n",
    "```\n",
    "\n",
    "Make a table and include all the query/snippet/ChatGPT outputs.ses sparse LU factorization, which is one form of sparse matrix factorization.*\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Summarize Findings\n",
    "\n",
    "Reflect on your observations. Consider the following guiding questions:\n",
    "\n",
    "**Query length:** How long should a query be to get useful results? Did shorter keyword queries work as well as longer, descriptive ones?\n",
    "\n",
    "**Documentation coverage:** Were some parts of the SciPy documentation more complete than others? How did that affect retrieval quality?\n",
    "\n",
    "**Exact vs. fuzzy matching:** Did you need to match words exactly (e.g., `\"LU decomposition\"` vs. `\"factorization\"`) to get good results? Give one example where exact matching was not required, and one where it failed.\n",
    "\n",
    "**Variation across query types:** Which query styles (conceptual, API-specific, natural language, edge cases) gave the most helpful results? Which tended to misfire?\n",
    "\n",
    "**Role of ChatGPT:** How reliable was ChatGPT as a relevance judge? Did it agree with other methods, such as manual check, ChatGPT check, notes)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bf361-125b-42be-ad3e-157744dc8244",
   "metadata": {},
   "source": [
    "\n",
    "### Questions for Discussion\n",
    "\n",
    "* Why might we use **different models** for code and for documentation?\n",
    "* Why might we choose **different chunk lengths** for code vs. docs?\n",
    "* What are the potential **advantages or trade-offs** of alternative strategies?\n",
    "\n",
    "*In your answers, consider the task requirements, the size of the corpus, and the amount of compute available (especially when using free models).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edafb26-4655-4612-8c66-d0ff4c5b4318",
   "metadata": {},
   "source": [
    "# Part 3 – Quantitative Evaluation\n",
    "\n",
    "We now want to **measure retrieval quality** (how well relevant code/doc chunks are retrieved) and **answer quality** (how grounded the model’s response is).\n",
    "\n",
    "---\n",
    "\n",
    "## Precision & Recall\n",
    "\n",
    "* **Precision\\@k**: fraction of the top *k* retrieved chunks that are relevant.\n",
    "\n",
    "  $$\n",
    "  \\text{Precision@k} = \\frac{\\# \\text{ relevant chunks in top k}}{k}\n",
    "  $$\n",
    "\n",
    "* **Recall\\@k**: fraction of all relevant chunks that appear in the top *k*.\n",
    "\n",
    "  $$\n",
    "  \\text{Recall@k} = \\frac{\\# \\text{ relevant chunks in top k}}{\\# \\text{ total relevant chunks}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## How to Evaluate\n",
    "\n",
    "1. Pick **3–5 SciPy-focused queries** (function names, error cases, natural language).\n",
    "2. For each query:\n",
    "\n",
    "   * Record the **retrieved top-k chunks**.\n",
    "   * Label which chunks are **relevant** (contain the needed function/definition/example).\n",
    "   * Compute precision\\@k and recall\\@k for k = 1, 3, 5.\n",
    "   * Collect the model’s answer.\n",
    "   * Assign an **answer relevance score (0/1)**:\n",
    "\n",
    "     * **1** = grounded in retrieved code/docs,\n",
    "     * **0** = hallucinated/unrelated.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Table with SciPy Queries\n",
    "\n",
    "| Query                                             | P\\@1 | P\\@3 | P\\@5 | R\\@5 | Answer Relevant? (0/1) | Notes                                  |\n",
    "| ------------------------------------------------- | ---- | ---- | ---- | ---- | ---------------------- | -------------------------------------- |\n",
    "| sparse.linalg.cg                                | 1    | 1    | 0.8  | 0.8  | 1                      | Correctly retrieved conjugate grad fn  |\n",
    "| fft convolution example                        | 1    | 0.67 | 0.8  | 0.8  | 1                      | Good examples in signal.fft docs       |\n",
    "| optimize.minimize with bounds                  | 1    | 1    | 1    | 1    | 1                      | Direct hit in optimization tutorial    |\n",
    "| stats.ttest_ind                                 | 0    | 0.33 | 0.4  | 0.4  | 0                      | Retrieved partial, model filled in     |\n",
    "| *How do I solve a sparse linear system in SciPy?* | 1    | 0.67 | 0.8  | 0.8  | 1                      | Found references to `spsolve` and `cg` |\n",
    "\n",
    "\n",
    "# Free or paid? It's up to you\n",
    "Given that our assessment is not very long, a free way of assessing the output is to just use the ChatGPT free interface. But, if you are so inclined, you are of course welcome to pay for the OpenAI API (e.g. `gpt-4o-mini`). It isn't very expensive, and can help you practice setting up more automated testbeds.  Here is the process for doing that:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def ask_llm(user_query, retrieved_chunks):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert on the following codebase.\n",
    "    Answer the question using only the provided code context.\n",
    "\n",
    "    Code context:\n",
    "    {retrieved_chunks}\n",
    "\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd1c40-8dfb-4e8b-983d-ef8d9d6008d5",
   "metadata": {},
   "source": [
    "# Part 4 – Adding Code Retrieval and Evaluation\n",
    "\n",
    "Now that you have embeddings for documentation, we will extend the system to also include **code embeddings**. This will allow the retriever to pull both documentation chunks *and* code chunks when answering a query.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1. Train & Add Code Embeddings\n",
    "\n",
    "1. Use the same embedding workflow as before, but now applied to **code chunks** (functions, classes, or 50–80 line segments).\n",
    "2. Store these in your vector index alongside your documentation embeddings. \n",
    "\n",
    "---\n",
    "\n",
    "## Step 2. Qualitative Assessment (Code Only)\n",
    "\n",
    "As you did earlier for documentation:\n",
    "\n",
    "1. Pick **3–5 queries** that are code-oriented (e.g., function names, error messages, “how do I call X?”).\n",
    "2. Retrieve the top chunks from the **code index** only.\n",
    "3. Evaluate qualitatively:\n",
    "\n",
    "   * Do the retrieved chunks contain the right functions/classes?\n",
    "   * Are signatures, docstrings, and examples useful?\n",
    "   * Where does it fail (e.g., missing, too long, irrelevant)?\n",
    "\n",
    "Keep brief notes for each query.\n",
    " \n",
    "\n",
    "## Step 3.  Quantitative Evaluation with Fusion\n",
    "\n",
    "Now we want to combine results from **doc retrieval** and **code retrieval** into a single ranked list. The process is:\n",
    "\n",
    "1. **Retrieve a large candidate pool**\n",
    "\n",
    "   * If your original top-k was, say, `k=5`, now retrieve about `10×k` (e.g., 50) from *each* index (docs and code).\n",
    "   * This ensures you don’t miss relevant results that would otherwise be cut off.\n",
    "\n",
    "2. **Compute fusion scores**\n",
    "\n",
    "   * **Reciprocal Rank Fusion (RRF):**\n",
    "\n",
    "     $$\n",
    "     \\text{RRF}(d) = \\sum_{i \\in \\{\\text{doc, code}\\}} \\frac{1}{k_0 + \\text{rank}_i(d)}, \\quad k_0 \\in [60,100]\n",
    "     $$\n",
    "\n",
    "     This combines the rankings from both sources.\n",
    "   * **(Optional)** Instead of RRF, you can use a **weighted similarity score**:\n",
    "\n",
    "     $$\n",
    "     \\text{score}(d) = 0.6 \\cdot \\text{score}_{\\text{code}}(d) + 0.4 \\cdot \\text{score}_{\\text{doc}}(d)\n",
    "     $$\n",
    "\n",
    "     Use this if your queries are often function- or symbol-heavy.\n",
    "\n",
    "3. **Re-rank and select final top-k**\n",
    "\n",
    "   * After computing scores for all candidates, sort them.\n",
    "   * Keep the best `k` (e.g., 5) as your fused retrieval output.\n",
    "\n",
    "\n",
    "\n",
    "## Step 4. Build Your Evaluation Table\n",
    "\n",
    "As before, compute **Precision\\@k** and note whether the **answer is grounded**. This time evaluate both fused retrieval methods (RRF and/or weighted sum).\n",
    "\n",
    "| Query                                      | P\\@1 | P\\@3 | P\\@5 | Answer Relevant? (0/1) | Notes                                               |\n",
    "| ------------------------------------------ | ---- | ---- | ---- | ---------------------- | --------------------------------------------------- |\n",
    "| `sparse.linalg.cg`                         | 1    | 1    | 0.8  | 1                      | Found correct function signature                    |\n",
    "| `fft convolution example`                  | 1    | 0.67 | 0.8  | 1                      | Docs + code both retrieved                          |\n",
    "| *How do I solve a sparse system in SciPy?* | 1    | 0.67 | 0.8  | 1                      | Fused retrieval covered both `spsolve` and tutorial |\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f116b4c-afa7-4cad-910f-c5c395950e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
